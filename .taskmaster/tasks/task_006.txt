# Task ID: 6
# Title: Integrate AI Grading with Gemini API
# Status: done
# Dependencies: 5
# Priority: high
# Description: Implement AI grading system using Google Gemini API to analyze submissions against user-defined rubrics
# Details:
Create Supabase Edge Function 'gradeSubmission' in TypeScript. Function triggered when submission status is 'pending' and referee_type is 'ai'. Implement pre-checks before API call: word count for documents using simple regex, parse URLs to check if accessible, OCR for images if needed. Construct Gemini API prompt with rubric from goal and submission content. Sanitize user rubric to prevent prompt injection. For MVP, focus on quantitative checks: document word count, GitHub commit verification, number recognition in images. Parse Gemini response to extract pass/fail verdict and confidence score. Update submission record with ai_analysis_result, status, and feedback. Implement retry logic for API failures. Add cost optimization by checking simple criteria first. Store API usage metrics for monitoring

# Test Strategy:
Test with various submission types and rubrics. Verify prompt injection prevention works. Test API error handling and retries. Ensure simple checks work without API call. Verify grading results are stored correctly. Test with edge cases like empty submissions or malformed rubrics

# Subtasks:
## 1. Set up gradeSubmission Edge Function and Database Trigger [done]
### Dependencies: None
### Description: Create the Supabase Edge Function scaffold and configure the database trigger to invoke it when submissions need AI grading
### Details:
Create a new TypeScript Edge Function named 'gradeSubmission' in the Supabase project. Set up the database trigger on the submissions table that fires when status is 'pending' AND referee_type is 'ai'. Configure proper authentication and permissions for the Edge Function to read submissions and goals data, and write back grading results. Set up the basic function signature with proper TypeScript types for the submission data structure. Include initial logging setup for monitoring function invocations.

## 2. Implement Pre-Processing and Cost-Saving Checks [done]
### Dependencies: 6.1
### Description: Build the pre-processing logic to fetch submission data and perform simple validation checks before making expensive API calls
### Details:
Fetch the complete submission record, associated goal, and rubric from the database. Implement simple pre-checks that can pass/fail submissions without API calls: word count validation using regex for text submissions, URL accessibility checks using HEAD requests, basic file format validation for uploaded documents. For images, determine if OCR is needed based on rubric requirements. Create a pre-check results object that determines if API call is necessary. Log all pre-check results for cost analysis. Return early with verdict if simple checks are conclusive.

## 3. Integrate Gemini API with Secure Prompt Construction [done]
### Dependencies: 6.2
### Description: Implement the core Gemini API integration with proper prompt engineering and security sanitization
### Details:
Set up Gemini API client with proper authentication using environment variables. Implement prompt sanitization function to prevent injection attacks by escaping special characters and limiting rubric length. Construct the AI prompt using a template that includes: the user's rubric, submission content (text, URL, or OCR results), and clear instructions for quantitative analysis. For MVP, focus prompts on measurable criteria like word counts, commit verification, and numerical targets. Include system prompts that enforce structured output format. Implement the API call with proper timeout and error handling.

## 4. Parse AI Response and Update Database [done]
### Dependencies: 6.3
### Description: Implement logic to parse the Gemini API response and update the submission record with grading results
### Details:
Create a robust parser for the Gemini response that extracts: pass/fail verdict (boolean), confidence score (0-100), and detailed feedback text. Handle various response formats since AI output can be unstructured. Implement fallback parsing strategies if initial parsing fails. Update the submission record with: ai_analysis_result (full response), status ('approved' or 'rejected'), graded_at timestamp, and feedback field with parsed explanation. Ensure database update is atomic and handles concurrent modifications. Log the parsing success rate for monitoring.

## 5. Implement Error Handling and Usage Monitoring [done]
### Dependencies: 6.4
### Description: Add comprehensive error handling, retry mechanisms, and usage tracking for cost optimization
### Details:
Implement exponential backoff retry logic for Gemini API failures (rate limits, network errors, timeouts). Set maximum retry attempts to 3 with increasing delays. Create detailed error logging that captures: error type, submission ID, retry count, and API response codes. Implement a dead letter queue pattern for submissions that fail after all retries. Track API usage metrics including: tokens consumed, processing time, cost per submission, and success/failure rates. Store metrics in a dedicated analytics table for dashboard reporting. Add circuit breaker pattern to prevent cascade failures during API outages.

